"""
ë¼ë„¤ì¦ˆ(Laneige) @cosme ë¶„ì„ íŒŒì´í”„ë¼ì¸ ë©”ì¸ ì‹¤í–‰ íŒŒì¼

ì‚¬ìš©ë²•:
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (í¬ë¡¤ë§ + ì €ì¥ + ë¶„ì„)
    python main.py

    # í¬ë¡¤ë§ë§Œ ì‹¤í–‰
    python main.py --crawl-only

    # ë¶„ì„ë§Œ ì‹¤í–‰ (ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©)
    python main.py --analyze-only

    # íŠ¹ì • LLM í”„ë¡œë°”ì´ë” ì‚¬ìš©
    python main.py --provider openai
    python main.py --provider anthropic
    python main.py --provider google
"""
import sys
import argparse
from datetime import datetime
from pathlib import Path

from loguru import logger

from config import config, LLMProvider, OUTPUT_DIR
from crawler.cosme_crawler import CosmeCrawler
from database.storage import ParquetStorage
from models.model_factory import ModelFactory
from analysis.llm_analyzer import LLMAnalyzer


# ë¡œê¹… ì„¤ì •
LOG_DIR = Path(__file__).parent / "logs"
LOG_DIR.mkdir(exist_ok=True)

logger.remove()
logger.add(sys.stderr, level="INFO", format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{message}</cyan>")
logger.add(
    LOG_DIR / "main_{time:YYYY-MM-DD}.log",
    rotation="1 day",
    retention="30 days",
    level="DEBUG"
)


def run_crawl(include_reviews: bool = True) -> tuple[list, list]:
    """í¬ë¡¤ë§ ì‹¤í–‰"""
    logger.info("=" * 60)
    logger.info("ğŸ•·ï¸  Starting @cosme Crawling")
    logger.info("=" * 60)
    
    crawler = CosmeCrawler()
    products, reviews = crawler.crawl_all(include_reviews=include_reviews)
    
    if not products:
        logger.warning("No products were crawled")
        return [], []
    
    # ì €ì¥
    storage = ParquetStorage()
    
    products_dict = crawler.to_dict_list(products)
    storage.save_products(products_dict)
    
    # ë­í‚¹ íˆìŠ¤í† ë¦¬ ì €ì¥
    rankings = storage.extract_rankings_from_products(products_dict)
    storage.save_rankings(rankings)
    
    if reviews:
        reviews_dict = crawler.to_dict_list(reviews)
        storage.save_reviews(reviews_dict)
    
    logger.info(f"âœ… Crawling complete: {len(products)} products, {len(reviews)} reviews")
    
    return products_dict, reviews if reviews else []


def run_analysis(provider: LLMProvider = None) -> Path:
    """LLM ë¶„ì„ ì‹¤í–‰"""
    logger.info("=" * 60)
    logger.info("ğŸ¤– Starting LLM Analysis")
    logger.info("=" * 60)
    
    analyzer = LLMAnalyzer(provider=provider)
    
    # ë°ì´í„° í†µê³„ í™•ì¸
    stats = analyzer.storage.get_statistics()
    logger.info(f"Data statistics: {stats}")
    
    if not stats.get("products_file_exists"):
        logger.error("No product data found. Run crawling first.")
        return None
    
    # ë¶„ì„ ì‹¤í–‰
    report_path = analyzer.run_full_analysis()
    
    logger.info(f"âœ… Analysis complete. Report saved to: {report_path}")
    
    return report_path


def show_statistics():
    """ì €ì¥ëœ ë°ì´í„° í†µê³„ í‘œì‹œ"""
    storage = ParquetStorage()
    stats = storage.get_statistics()
    
    print("\n" + "=" * 60)
    print("ğŸ“Š Data Statistics")
    print("=" * 60)
    
    for key, value in stats.items():
        print(f"  {key}: {value}")
    
    print("=" * 60 + "\n")


def list_reports():
    """ì €ì¥ëœ ë¦¬í¬íŠ¸ ëª©ë¡ í‘œì‹œ"""
    print("\n" + "=" * 60)
    print("ğŸ“„ Saved Reports")
    print("=" * 60)
    
    report_files = sorted(OUTPUT_DIR.glob("*.txt"), reverse=True)
    
    if not report_files:
        print("  No reports found")
    else:
        for f in report_files[:20]:  # ìµœê·¼ 20ê°œë§Œ
            size = f.stat().st_size / 1024
            mtime = datetime.fromtimestamp(f.stat().st_mtime).strftime("%Y-%m-%d %H:%M")
            print(f"  {f.name} ({size:.1f} KB, {mtime})")
    
    print("=" * 60 + "\n")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="ë¼ë„¤ì¦ˆ(Laneige) @cosme ë¶„ì„ íŒŒì´í”„ë¼ì¸",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  python main.py                    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
  python main.py --crawl-only       # í¬ë¡¤ë§ë§Œ ì‹¤í–‰
  python main.py --analyze-only     # ë¶„ì„ë§Œ ì‹¤í–‰
  python main.py --provider openai  # OpenAI GPTë¡œ ë¶„ì„
  python main.py --gpu 0            # GPU 0ë²ˆ ì‚¬ìš© (ë¡œì»¬ ëª¨ë¸)
  python main.py --gpu 0,1          # GPU 0,1ë²ˆ ì‚¬ìš© (ë‹¤ì¤‘ GPU)
  python main.py --stats            # ë°ì´í„° í†µê³„ í™•ì¸
  python main.py --list-reports     # ì €ì¥ëœ ë¦¬í¬íŠ¸ ëª©ë¡
        """
    )
    
    parser.add_argument(
        "--crawl-only",
        action="store_true",
        help="í¬ë¡¤ë§ë§Œ ì‹¤í–‰ (ë¶„ì„ ìƒëµ)"
    )
    parser.add_argument(
        "--analyze-only",
        action="store_true",
        help="ë¶„ì„ë§Œ ì‹¤í–‰ (ê¸°ì¡´ ë°ì´í„° ì‚¬ìš©)"
    )
    parser.add_argument(
        "--no-reviews",
        action="store_true",
        help="ë¦¬ë·° í¬ë¡¤ë§ ìƒëµ"
    )
    parser.add_argument(
        "--provider",
        choices=["qwen", "openai", "anthropic", "google"],
        default="qwen",
        help="LLM í”„ë¡œë°”ì´ë” ì„ íƒ (ê¸°ë³¸: qwen)"
    )
    parser.add_argument(
        "--stats",
        action="store_true",
        help="ì €ì¥ëœ ë°ì´í„° í†µê³„ í‘œì‹œ"
    )
    parser.add_argument(
        "--list-reports",
        action="store_true",
        help="ì €ì¥ëœ ë¦¬í¬íŠ¸ ëª©ë¡ í‘œì‹œ"
    )
    parser.add_argument(
        "--list-providers",
        action="store_true",
        help="ì‚¬ìš© ê°€ëŠ¥í•œ LLM í”„ë¡œë°”ì´ë” ëª©ë¡"
    )
    parser.add_argument(
        "--gpu",
        type=str,
        default=None,
        help="ì‚¬ìš©í•  GPU ID (ì˜ˆ: '0', '0,1', '1,2,3'). ë¡œì»¬ ëª¨ë¸(qwen)ì—ë§Œ ì ìš©"
    )
    
    args = parser.parse_args()
    
    # ì •ë³´ í‘œì‹œ ì˜µì…˜
    if args.stats:
        show_statistics()
        return
    
    if args.list_reports:
        list_reports()
        return
    
    if args.list_providers:
        print("\nì‚¬ìš© ê°€ëŠ¥í•œ LLM í”„ë¡œë°”ì´ë”:")
        for p in ModelFactory.list_providers():
            print(f"  - {p}")
        return
    
    # LLM í”„ë¡œë°”ì´ë” ì„¤ì •
    provider = LLMProvider(args.provider)
    
    # GPU ì„¤ì • (ë¡œì»¬ ëª¨ë¸ìš©)
    if args.gpu is not None:
        config.llm.gpu_ids = args.gpu
        logger.info(f"GPU setting: {args.gpu}")
    
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                                                           â•‘
    â•‘   ğŸ§´ ë¼ë„¤ì¦ˆ(Laneige) @cosme ë¶„ì„ íŒŒì´í”„ë¼ì¸              â•‘
    â•‘                                                           â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)
    
    start_time = datetime.now()
    logger.info(f"Pipeline started at {start_time}")
    
    try:
        if args.analyze_only:
            # ë¶„ì„ë§Œ ì‹¤í–‰
            report_path = run_analysis(provider)
        
        elif args.crawl_only:
            # í¬ë¡¤ë§ë§Œ ì‹¤í–‰
            run_crawl(include_reviews=not args.no_reviews)
        
        else:
            # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            products, reviews = run_crawl(include_reviews=not args.no_reviews)
            
            if products:
                report_path = run_analysis(provider)
            else:
                logger.warning("Skipping analysis due to no data")
        
        end_time = datetime.now()
        duration = end_time - start_time
        
        logger.info("=" * 60)
        logger.info(f"ğŸ‰ Pipeline completed successfully!")
        logger.info(f"   Duration: {duration}")
        logger.info("=" * 60)
        
    except KeyboardInterrupt:
        logger.warning("Pipeline interrupted by user")
        sys.exit(1)
    
    except Exception as e:
        logger.error(f"Pipeline failed: {e}")
        raise


if __name__ == "__main__":
    main()

